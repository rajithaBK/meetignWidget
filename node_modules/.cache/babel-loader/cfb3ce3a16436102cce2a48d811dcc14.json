{"ast":null,"code":"\"use strict\";\n\nvar _regeneratorRuntime = require(\"C:\\\\Users\\\\rkanthet\\\\Documents\\\\test_window_object\\\\node_modules\\\\@babel\\\\runtime\\\\regenerator\\\\index.js\");\n\nvar _asyncToGenerator = require(\"C:/Users/rkanthet/Documents/test_window_object/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\n\nvar _classCallCheck = require(\"C:/Users/rkanthet/Documents/test_window_object/node_modules/@babel/runtime/helpers/classCallCheck.js\").default;\n\nvar _createClass = require(\"C:/Users/rkanthet/Documents/test_window_object/node_modules/@babel/runtime/helpers/createClass.js\").default;\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.AbstractTokenizer = void 0;\n\nvar peek_readable_1 = require(\"peek-readable\");\n/**\r\n * Core tokenizer\r\n */\n\n\nvar AbstractTokenizer = /*#__PURE__*/function () {\n  function AbstractTokenizer(fileInfo) {\n    _classCallCheck(this, AbstractTokenizer);\n\n    /**\r\n     * Tokenizer-stream position\r\n     */\n    this.position = 0;\n    this.numBuffer = new Uint8Array(8);\n    this.fileInfo = fileInfo ? fileInfo : {};\n  }\n  /**\r\n   * Read a token from the tokenizer-stream\r\n   * @param token - The token to read\r\n   * @param position - If provided, the desired position in the tokenizer-stream\r\n   * @returns Promise with token data\r\n   */\n\n\n  _createClass(AbstractTokenizer, [{\n    key: \"readToken\",\n    value: function () {\n      var _readToken = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(token) {\n        var position,\n            uint8Array,\n            len,\n            _args = arguments;\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                position = _args.length > 1 && _args[1] !== undefined ? _args[1] : this.position;\n                uint8Array = Buffer.alloc(token.len);\n                _context.next = 4;\n                return this.readBuffer(uint8Array, {\n                  position: position\n                });\n\n              case 4:\n                len = _context.sent;\n\n                if (!(len < token.len)) {\n                  _context.next = 7;\n                  break;\n                }\n\n                throw new peek_readable_1.EndOfStreamError();\n\n              case 7:\n                return _context.abrupt(\"return\", token.get(uint8Array, 0));\n\n              case 8:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function readToken(_x) {\n        return _readToken.apply(this, arguments);\n      }\n\n      return readToken;\n    }()\n    /**\r\n     * Peek a token from the tokenizer-stream.\r\n     * @param token - Token to peek from the tokenizer-stream.\r\n     * @param position - Offset where to begin reading within the file. If position is null, data will be read from the current file position.\r\n     * @returns Promise with token data\r\n     */\n\n  }, {\n    key: \"peekToken\",\n    value: function () {\n      var _peekToken = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(token) {\n        var position,\n            uint8Array,\n            len,\n            _args2 = arguments;\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                position = _args2.length > 1 && _args2[1] !== undefined ? _args2[1] : this.position;\n                uint8Array = Buffer.alloc(token.len);\n                _context2.next = 4;\n                return this.peekBuffer(uint8Array, {\n                  position: position\n                });\n\n              case 4:\n                len = _context2.sent;\n\n                if (!(len < token.len)) {\n                  _context2.next = 7;\n                  break;\n                }\n\n                throw new peek_readable_1.EndOfStreamError();\n\n              case 7:\n                return _context2.abrupt(\"return\", token.get(uint8Array, 0));\n\n              case 8:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2, this);\n      }));\n\n      function peekToken(_x2) {\n        return _peekToken.apply(this, arguments);\n      }\n\n      return peekToken;\n    }()\n    /**\r\n     * Read a numeric token from the stream\r\n     * @param token - Numeric token\r\n     * @returns Promise with number\r\n     */\n\n  }, {\n    key: \"readNumber\",\n    value: function () {\n      var _readNumber = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(token) {\n        var len;\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                _context3.next = 2;\n                return this.readBuffer(this.numBuffer, {\n                  length: token.len\n                });\n\n              case 2:\n                len = _context3.sent;\n\n                if (!(len < token.len)) {\n                  _context3.next = 5;\n                  break;\n                }\n\n                throw new peek_readable_1.EndOfStreamError();\n\n              case 5:\n                return _context3.abrupt(\"return\", token.get(this.numBuffer, 0));\n\n              case 6:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n\n      function readNumber(_x3) {\n        return _readNumber.apply(this, arguments);\n      }\n\n      return readNumber;\n    }()\n    /**\r\n     * Read a numeric token from the stream\r\n     * @param token - Numeric token\r\n     * @returns Promise with number\r\n     */\n\n  }, {\n    key: \"peekNumber\",\n    value: function () {\n      var _peekNumber = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(token) {\n        var len;\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                _context4.next = 2;\n                return this.peekBuffer(this.numBuffer, {\n                  length: token.len\n                });\n\n              case 2:\n                len = _context4.sent;\n\n                if (!(len < token.len)) {\n                  _context4.next = 5;\n                  break;\n                }\n\n                throw new peek_readable_1.EndOfStreamError();\n\n              case 5:\n                return _context4.abrupt(\"return\", token.get(this.numBuffer, 0));\n\n              case 6:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n\n      function peekNumber(_x4) {\n        return _peekNumber.apply(this, arguments);\n      }\n\n      return peekNumber;\n    }()\n    /**\r\n     * Ignore number of bytes, advances the pointer in under tokenizer-stream.\r\n     * @param length - Number of bytes to ignore\r\n     * @return resolves the number of bytes ignored, equals length if this available, otherwise the number of bytes available\r\n     */\n\n  }, {\n    key: \"ignore\",\n    value: function () {\n      var _ignore = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee5(length) {\n        var bytesLeft;\n        return _regeneratorRuntime.wrap(function _callee5$(_context5) {\n          while (1) {\n            switch (_context5.prev = _context5.next) {\n              case 0:\n                if (!(this.fileInfo.size !== undefined)) {\n                  _context5.next = 5;\n                  break;\n                }\n\n                bytesLeft = this.fileInfo.size - this.position;\n\n                if (!(length > bytesLeft)) {\n                  _context5.next = 5;\n                  break;\n                }\n\n                this.position += bytesLeft;\n                return _context5.abrupt(\"return\", bytesLeft);\n\n              case 5:\n                this.position += length;\n                return _context5.abrupt(\"return\", length);\n\n              case 7:\n              case \"end\":\n                return _context5.stop();\n            }\n          }\n        }, _callee5, this);\n      }));\n\n      function ignore(_x5) {\n        return _ignore.apply(this, arguments);\n      }\n\n      return ignore;\n    }()\n  }, {\n    key: \"close\",\n    value: function () {\n      var _close = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee6() {\n        return _regeneratorRuntime.wrap(function _callee6$(_context6) {\n          while (1) {\n            switch (_context6.prev = _context6.next) {\n              case 0:\n              case \"end\":\n                return _context6.stop();\n            }\n          }\n        }, _callee6);\n      }));\n\n      function close() {\n        return _close.apply(this, arguments);\n      }\n\n      return close;\n    }()\n  }, {\n    key: \"normalizeOptions\",\n    value: function normalizeOptions(uint8Array, options) {\n      if (options && options.position !== undefined && options.position < this.position) {\n        throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\n      }\n\n      if (options) {\n        return {\n          mayBeLess: options.mayBeLess === true,\n          offset: options.offset ? options.offset : 0,\n          length: options.length ? options.length : uint8Array.length - (options.offset ? options.offset : 0),\n          position: options.position ? options.position : this.position\n        };\n      }\n\n      return {\n        mayBeLess: false,\n        offset: 0,\n        length: uint8Array.length,\n        position: this.position\n      };\n    }\n  }]);\n\n  return AbstractTokenizer;\n}();\n\nexports.AbstractTokenizer = AbstractTokenizer;","map":{"version":3,"sources":["C:/Users/rkanthet/Documents/test_window_object/node_modules/strtok3/lib/AbstractTokenizer.js"],"names":["Object","defineProperty","exports","value","AbstractTokenizer","peek_readable_1","require","fileInfo","position","numBuffer","Uint8Array","token","uint8Array","Buffer","alloc","len","readBuffer","EndOfStreamError","get","peekBuffer","length","size","undefined","bytesLeft","options","Error","mayBeLess","offset"],"mappings":"AAAA;;;;;;;;;;AACAA,MAAM,CAACC,cAAP,CAAsBC,OAAtB,EAA+B,YAA/B,EAA6C;AAAEC,EAAAA,KAAK,EAAE;AAAT,CAA7C;AACAD,OAAO,CAACE,iBAAR,GAA4B,KAAK,CAAjC;;AACA,IAAMC,eAAe,GAAGC,OAAO,CAAC,eAAD,CAA/B;AACA;AACA;AACA;;;IACMF,iB;AACF,6BAAYG,QAAZ,EAAsB;AAAA;;AAClB;AACR;AACA;AACQ,SAAKC,QAAL,GAAgB,CAAhB;AACA,SAAKC,SAAL,GAAiB,IAAIC,UAAJ,CAAe,CAAf,CAAjB;AACA,SAAKH,QAAL,GAAgBA,QAAQ,GAAGA,QAAH,GAAc,EAAtC;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;;;;;;gFACI,iBAAgBI,KAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAuBH,gBAAAA,QAAvB,2DAAkC,KAAKA,QAAvC;AACUI,gBAAAA,UADV,GACuBC,MAAM,CAACC,KAAP,CAAaH,KAAK,CAACI,GAAnB,CADvB;AAAA;AAAA,uBAEsB,KAAKC,UAAL,CAAgBJ,UAAhB,EAA4B;AAAEJ,kBAAAA,QAAQ,EAARA;AAAF,iBAA5B,CAFtB;;AAAA;AAEUO,gBAAAA,GAFV;;AAAA,sBAGQA,GAAG,GAAGJ,KAAK,CAACI,GAHpB;AAAA;AAAA;AAAA;;AAAA,sBAIc,IAAIV,eAAe,CAACY,gBAApB,EAJd;;AAAA;AAAA,iDAKWN,KAAK,CAACO,GAAN,CAAUN,UAAV,EAAsB,CAAtB,CALX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;AAOA;AACJ;AACA;AACA;AACA;AACA;;;;;gFACI,kBAAgBD,KAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAuBH,gBAAAA,QAAvB,8DAAkC,KAAKA,QAAvC;AACUI,gBAAAA,UADV,GACuBC,MAAM,CAACC,KAAP,CAAaH,KAAK,CAACI,GAAnB,CADvB;AAAA;AAAA,uBAEsB,KAAKI,UAAL,CAAgBP,UAAhB,EAA4B;AAAEJ,kBAAAA,QAAQ,EAARA;AAAF,iBAA5B,CAFtB;;AAAA;AAEUO,gBAAAA,GAFV;;AAAA,sBAGQA,GAAG,GAAGJ,KAAK,CAACI,GAHpB;AAAA;AAAA;AAAA;;AAAA,sBAIc,IAAIV,eAAe,CAACY,gBAApB,EAJd;;AAAA;AAAA,kDAKWN,KAAK,CAACO,GAAN,CAAUN,UAAV,EAAsB,CAAtB,CALX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;AAOA;AACJ;AACA;AACA;AACA;;;;;iFACI,kBAAiBD,KAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBACsB,KAAKK,UAAL,CAAgB,KAAKP,SAArB,EAAgC;AAAEW,kBAAAA,MAAM,EAAET,KAAK,CAACI;AAAhB,iBAAhC,CADtB;;AAAA;AACUA,gBAAAA,GADV;;AAAA,sBAEQA,GAAG,GAAGJ,KAAK,CAACI,GAFpB;AAAA;AAAA;AAAA;;AAAA,sBAGc,IAAIV,eAAe,CAACY,gBAApB,EAHd;;AAAA;AAAA,kDAIWN,KAAK,CAACO,GAAN,CAAU,KAAKT,SAAf,EAA0B,CAA1B,CAJX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;AAMA;AACJ;AACA;AACA;AACA;;;;;iFACI,kBAAiBE,KAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBACsB,KAAKQ,UAAL,CAAgB,KAAKV,SAArB,EAAgC;AAAEW,kBAAAA,MAAM,EAAET,KAAK,CAACI;AAAhB,iBAAhC,CADtB;;AAAA;AACUA,gBAAAA,GADV;;AAAA,sBAEQA,GAAG,GAAGJ,KAAK,CAACI,GAFpB;AAAA;AAAA;AAAA;;AAAA,sBAGc,IAAIV,eAAe,CAACY,gBAApB,EAHd;;AAAA;AAAA,kDAIWN,KAAK,CAACO,GAAN,CAAU,KAAKT,SAAf,EAA0B,CAA1B,CAJX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;AAMA;AACJ;AACA;AACA;AACA;;;;;6EACI,kBAAaW,MAAb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBACQ,KAAKb,QAAL,CAAcc,IAAd,KAAuBC,SAD/B;AAAA;AAAA;AAAA;;AAEcC,gBAAAA,SAFd,GAE0B,KAAKhB,QAAL,CAAcc,IAAd,GAAqB,KAAKb,QAFpD;;AAAA,sBAGYY,MAAM,GAAGG,SAHrB;AAAA;AAAA;AAAA;;AAIY,qBAAKf,QAAL,IAAiBe,SAAjB;AAJZ,kDAKmBA,SALnB;;AAAA;AAQI,qBAAKf,QAAL,IAAiBY,MAAjB;AARJ,kDASWA,MATX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;;;;4EAWA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,O;;;;;;;;;;WAGA,0BAAiBR,UAAjB,EAA6BY,OAA7B,EAAsC;AAClC,UAAIA,OAAO,IAAIA,OAAO,CAAChB,QAAR,KAAqBc,SAAhC,IAA6CE,OAAO,CAAChB,QAAR,GAAmB,KAAKA,QAAzE,EAAmF;AAC/E,cAAM,IAAIiB,KAAJ,CAAU,uEAAV,CAAN;AACH;;AACD,UAAID,OAAJ,EAAa;AACT,eAAO;AACHE,UAAAA,SAAS,EAAEF,OAAO,CAACE,SAAR,KAAsB,IAD9B;AAEHC,UAAAA,MAAM,EAAEH,OAAO,CAACG,MAAR,GAAiBH,OAAO,CAACG,MAAzB,GAAkC,CAFvC;AAGHP,UAAAA,MAAM,EAAEI,OAAO,CAACJ,MAAR,GAAiBI,OAAO,CAACJ,MAAzB,GAAmCR,UAAU,CAACQ,MAAX,IAAqBI,OAAO,CAACG,MAAR,GAAiBH,OAAO,CAACG,MAAzB,GAAkC,CAAvD,CAHxC;AAIHnB,UAAAA,QAAQ,EAAEgB,OAAO,CAAChB,QAAR,GAAmBgB,OAAO,CAAChB,QAA3B,GAAsC,KAAKA;AAJlD,SAAP;AAMH;;AACD,aAAO;AACHkB,QAAAA,SAAS,EAAE,KADR;AAEHC,QAAAA,MAAM,EAAE,CAFL;AAGHP,QAAAA,MAAM,EAAER,UAAU,CAACQ,MAHhB;AAIHZ,QAAAA,QAAQ,EAAE,KAAKA;AAJZ,OAAP;AAMH;;;;;;AAELN,OAAO,CAACE,iBAAR,GAA4BA,iBAA5B","sourcesContent":["\"use strict\";\r\nObject.defineProperty(exports, \"__esModule\", { value: true });\r\nexports.AbstractTokenizer = void 0;\r\nconst peek_readable_1 = require(\"peek-readable\");\r\n/**\r\n * Core tokenizer\r\n */\r\nclass AbstractTokenizer {\r\n    constructor(fileInfo) {\r\n        /**\r\n         * Tokenizer-stream position\r\n         */\r\n        this.position = 0;\r\n        this.numBuffer = new Uint8Array(8);\r\n        this.fileInfo = fileInfo ? fileInfo : {};\r\n    }\r\n    /**\r\n     * Read a token from the tokenizer-stream\r\n     * @param token - The token to read\r\n     * @param position - If provided, the desired position in the tokenizer-stream\r\n     * @returns Promise with token data\r\n     */\r\n    async readToken(token, position = this.position) {\r\n        const uint8Array = Buffer.alloc(token.len);\r\n        const len = await this.readBuffer(uint8Array, { position });\r\n        if (len < token.len)\r\n            throw new peek_readable_1.EndOfStreamError();\r\n        return token.get(uint8Array, 0);\r\n    }\r\n    /**\r\n     * Peek a token from the tokenizer-stream.\r\n     * @param token - Token to peek from the tokenizer-stream.\r\n     * @param position - Offset where to begin reading within the file. If position is null, data will be read from the current file position.\r\n     * @returns Promise with token data\r\n     */\r\n    async peekToken(token, position = this.position) {\r\n        const uint8Array = Buffer.alloc(token.len);\r\n        const len = await this.peekBuffer(uint8Array, { position });\r\n        if (len < token.len)\r\n            throw new peek_readable_1.EndOfStreamError();\r\n        return token.get(uint8Array, 0);\r\n    }\r\n    /**\r\n     * Read a numeric token from the stream\r\n     * @param token - Numeric token\r\n     * @returns Promise with number\r\n     */\r\n    async readNumber(token) {\r\n        const len = await this.readBuffer(this.numBuffer, { length: token.len });\r\n        if (len < token.len)\r\n            throw new peek_readable_1.EndOfStreamError();\r\n        return token.get(this.numBuffer, 0);\r\n    }\r\n    /**\r\n     * Read a numeric token from the stream\r\n     * @param token - Numeric token\r\n     * @returns Promise with number\r\n     */\r\n    async peekNumber(token) {\r\n        const len = await this.peekBuffer(this.numBuffer, { length: token.len });\r\n        if (len < token.len)\r\n            throw new peek_readable_1.EndOfStreamError();\r\n        return token.get(this.numBuffer, 0);\r\n    }\r\n    /**\r\n     * Ignore number of bytes, advances the pointer in under tokenizer-stream.\r\n     * @param length - Number of bytes to ignore\r\n     * @return resolves the number of bytes ignored, equals length if this available, otherwise the number of bytes available\r\n     */\r\n    async ignore(length) {\r\n        if (this.fileInfo.size !== undefined) {\r\n            const bytesLeft = this.fileInfo.size - this.position;\r\n            if (length > bytesLeft) {\r\n                this.position += bytesLeft;\r\n                return bytesLeft;\r\n            }\r\n        }\r\n        this.position += length;\r\n        return length;\r\n    }\r\n    async close() {\r\n        // empty\r\n    }\r\n    normalizeOptions(uint8Array, options) {\r\n        if (options && options.position !== undefined && options.position < this.position) {\r\n            throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\r\n        }\r\n        if (options) {\r\n            return {\r\n                mayBeLess: options.mayBeLess === true,\r\n                offset: options.offset ? options.offset : 0,\r\n                length: options.length ? options.length : (uint8Array.length - (options.offset ? options.offset : 0)),\r\n                position: options.position ? options.position : this.position\r\n            };\r\n        }\r\n        return {\r\n            mayBeLess: false,\r\n            offset: 0,\r\n            length: uint8Array.length,\r\n            position: this.position\r\n        };\r\n    }\r\n}\r\nexports.AbstractTokenizer = AbstractTokenizer;\r\n"]},"metadata":{},"sourceType":"script"}